# Kevin Walton â€” AI Evaluation & Technical Reasoning

I work at the intersection of **AI evaluation**, **prompt engineering**, and **clear technical reasoning**. My background blends hands-on troubleshooting, structured analysis, and a creative instinct for patterns â€” the kind that shows up in both good engineering and good storytelling.

I build small, focused evaluation samples that demonstrate how large language models behave under pressure: **ambiguity**, **bias**, **missing information**, and **constraint**.

My approach is methodical, evidence-driven, and grounded in reproducible workflows. I value clarity, uncertainty-awareness, and the discipline of testing one variable at a time.

---

## ğŸ” Evaluation Portfolio

These projects form a growing series of targeted evaluations, each designed to isolate a single behavioural dimension of LLMs.

### [llm-hallucination-missing-information](https://github.com/Hermeticpoet/llm-hallucination-missing-information)

Tests how models respond when key facts are absent or fictional.  
Includes raw outputs, analysis notes, and a concise summary.

### [llm-directional-bias-evaluation](https://github.com/Hermeticpoet/llm-directional-bias-evaluation)

Examines whether models drift toward a preferred framing when both sides are balanced.  
Uses symmetric prompt testing and structured comparisons.

More samples are planned â€” each small, sharp, and methodologically clean.

---

## ğŸ§  How I Work

I design evaluations using:

- âœ… Test matrices with clear expectations
- ğŸ“‚ Raw model outputs stored for transparency
- ğŸ“ Analysis notes capturing early observations
- ğŸ“Š Concise reports summarising behavioural patterns
- ğŸ” Reproducible workflows that others can follow

My goal is to make evaluation **understandable**, **inspectable**, and **grounded in evidence** rather than intuition.

---

## ğŸ›  Technical Foundations

I have practical experience with:

- JavaScript fundamentals
- API workflows
- zsh scripting
- Git and reproducible documentation
- Basic security analysis and OSINT tooling
- Model behaviour testing and prompt design

Iâ€™m continuously expanding these skills while keeping my focus on clarity and method.

---

## ğŸŒ± Roadmap

Upcoming evaluation samples:

- Instruction-following robustness
- Paraphrase consistency
- Safety and refusal behaviour
- Multi-step reasoning drift

Each will follow the same structure: **small**, **controlled**, and **easy to inspect**.

---

## ğŸ“Œ Pinned Projects

- **[llm-hallucination-missing-information](https://github.com/Hermeticpoet/llm-hallucination-missing-information)**  
  _Tests model behavior under missing or fictional facts_

- **[llm-directional-bias-evaluation](https://github.com/Hermeticpoet/llm-directional-bias-evaluation)**  
  _Evaluates framing bias using symmetric prompts_

- **[Prompt_Engineering_Portfolio](https://github.com/Hermeticpoet/Prompt_Engineering_Portfolio)**  
  _Jailbreak resistance, few-shot prompting, security stress tests_

- **[osint-dashboard-app](https://github.com/Hermeticpoet/osint-dashboard-app)**  
  _URL safety checker with SSL, domain, and phishing analysis_

- **[javascript-fundamentals-notes](https://github.com/Hermeticpoet/javascript-fundamentals-notes)**  
  _Refresher notes on JS concepts and syntax_

---

## ğŸ“ Courses Completed

- Google AI Essentials
- DeepLearning.AI Prompt Engineering for ChatGPT
- Kapsuun OSINT & Threat Intelligence
- More in progressâ€¦

---

## âœ¦ A Note on Identity

The name **Hermeticpoet** reflects the two halves of my work:  
the **precision of systems** and the **resonance of narrative**.  
Both matter when evaluating models that generate meaning.

---

## ğŸ“¬ Contact

If youâ€™re exploring AI evaluation, reproducible workflows, or structured prompt design, feel free to dive into the repos above â€” or reach out.

ğŸ“¬ [Email](mailto:kevywalton@outlook.com)
